{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `getindex(o::PyObject, s::AbstractString)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.\"s\"` instead of `o[\"s\"]`.\n",
      "│   caller = __init__() at PyCallJLD.jl:12\n",
      "└ @ PyCallJLD /opt/julia/packages/PyCallJLD/Tfc36/src/PyCallJLD.jl:12\n",
      "┌ Warning: `getindex(o::PyObject, s::AbstractString)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.\"s\"` instead of `o[\"s\"]`.\n",
      "│   caller = __init__() at PyCallJLD.jl:13\n",
      "└ @ PyCallJLD /opt/julia/packages/PyCallJLD/Tfc36/src/PyCallJLD.jl:13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.preprocessing.data.MinMaxScaler'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Images\n",
    "using JLD\n",
    "using PyCall\n",
    "using PyCallJLD\n",
    "using PyPlot\n",
    "using Statistics\n",
    "\n",
    "using ScikitLearn \n",
    "using ScikitLearn.CrossValidation: cross_val_score\n",
    "using ScikitLearn.GridSearch: GridSearchCV\n",
    "using ScikitLearn.Pipelines: Pipeline, FeatureUnion, make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "@sk_import datasets: load_breast_cancer\n",
    "@sk_import decomposition: PCA\n",
    "@sk_import ensemble: RandomForestClassifier\n",
    "@sk_import feature_selection: SelectPercentile\n",
    "@sk_import metrics: f1_score\n",
    "@sk_import metrics: make_scorer\n",
    "@sk_import preprocessing: PolynomialFeatures\n",
    "@sk_import preprocessing: MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use a Julia Machine Learning Model in Production\n",
    "\n",
    "Take advantage of: \n",
    "- docker (jupyter Julia, Python, R) container\n",
    "- scikitlearn.jl\n",
    "- genie.jl (Model View Controller (MVC) framework)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load(\"overviews.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with SCIKIT LEARN for Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load(\"scikit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get UCI ML Breast Cancer Wisconsin (Diagnostic) dataset\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "\n",
    "### Good Example of Classic Binary Classification dataset\n",
    "- 569 observations \n",
    "- 30 features\n",
    "-  Samples per class 212-Malignant(1) and 357-Benign(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer(); #load data as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer[\"data\"];\n",
    "cancer[\"feature_names\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data features values to X's\n",
    "X = cancer[\"data\"]\n",
    "size(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Target values to y\n",
    "y = cancer[\"target\"]\n",
    "size(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Feature Engineering\n",
    "\n",
    "We think the model may perform better with polynomial features like: $x^2$, xy and $y^2$\n",
    "\n",
    "Sometimes these feature combinations have more important impact than the original features. Note, they expand number of features 30 to 496 \n",
    "\n",
    "If the `interactions_only` option is not used, the number of produced features is:\n",
    "\n",
    "$$ \\#Features = N + N + \\frac{N \\times (N-1)}{2} + 1 $$\n",
    "\n",
    "E.g. for degree=30, it is 496; for degree=100, it is 5151."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Polynomial Features\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "size(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the raw data to see that we might need to scale it. \n",
    "X_poly[1:4,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data \n",
    "Scale the dat for better performance in subsequent models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute minimum and maximum on the training data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_poly)\n",
    "\n",
    "#rescale training data\n",
    "X_poly_scaled = scaler.transform(X_poly)\n",
    "size(X_poly_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the scaled Poly expanded traning data\n",
    "X_poly_scaled[1:4,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Reduce the Number of Important Features  -- Select Most Important Engineered Features\n",
    "\n",
    "Because we increased the number of features (30 to 496), now we can go back and select the features that have most\n",
    "importance using a selection tool called `SelectPrecentile`.\n",
    "\n",
    "SelectPercentile is a univariate feature selector which says what percentage of features to keep. \n",
    "\n",
    "Think Principal Component Analysis (PCA) for multivariate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = SelectPercentile(percentile=20)\n",
    "select.fit(X_poly_scaled, y) #need both scaled training and target for fit\n",
    "X_selected = select.transform(X_poly_scaled)\n",
    "size(X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the selected  expanded traning data of the most dominant features looks like column 2 from X_poly_selected \n",
    "# starts out\n",
    "X_selected[1:4,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feature Engineered Data Against Known Model : RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "There are several different algorithms that attempt to *blend* precision and recall to produce a single \"score.\"  Scikit-learn provides a number of other scalar scores that are useful for differing purposes (and other libraries are similar), but F1 score is one that is used very frequently.  It is simply:\n",
    "\n",
    "$$\\text{F1} = 2 \\times \\cfrac{precision \\times recall}{precision + recall}$$\n",
    "\n",
    "\n",
    "We get precision and recall from Confusion Table/Contingency Matrix entries \n",
    "\n",
    "\n",
    "Consider a binary problem though:\n",
    "\n",
    "| Predict/Actual | Positive | Negative |\n",
    "|----------------|----------|----------|\n",
    "| Positive       | some_val | some_val |\n",
    "| Negative       | some_val | some_val |\n",
    "\n",
    "\n",
    "\n",
    "Here, Precision is:\n",
    "\n",
    "\n",
    "$$\\text{Precision} = \\frac{true\\: positive}{true\\: positive + false\\: positive}$$\n",
    "\n",
    "Generalizing that to the multi-class case, the formula is as follows (for i being the index of the class):\n",
    "\n",
    "\n",
    "$$\\text{Precision}_{i} = \\cfrac{M_{ii}}{\\sum_i M_{ij}}$$\n",
    "\n",
    "\n",
    "And, Recall is:\n",
    "\n",
    "\n",
    "$$\\text{Recall} = \\frac{true\\: positive}{true\\: positive + false\\: negative}$$\n",
    "\n",
    "Generalizing that to the multi-class case:\n",
    "\n",
    "$$\\text{Recall}_{i} = \\cfrac{M_{ii}}{\\sum_j M_{ij}}$$\n",
    "\n",
    "\n",
    "F1 score can be generalized to multi-class models by averaging the F1 score across each class, counting only correct/incorrect per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(f1_score) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-valdiation Model Check\n",
    "#Cross-validation is to help train and test different groupings of the data so we ensure better model performance\n",
    "# reduce bias of data, help make sure we do not miss patterns or trends \n",
    "# identify overfitting\n",
    "\n",
    "cv_scores = cross_val_score(rfc, X_selected, y, scoring=scorer, cv = 5)# Stratified kfold done here. \n",
    "println(\" CV scores: \", cv_scores)\n",
    "println(\"Mean score: \", mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Using Pipelines - Bundle all Your Operations\n",
    "\n",
    "\n",
    "A pipeline is simply an abstraction in scikit-learn to bundle together steps like those used above into a single model interface, following the same APIs as a model itself.  A particular pipeline is likely to be somewhat domain specific in that you may learn that those particular steps are useful for e.g. cancer data, but not as useful for data with very different characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load(\"pipeline-diagram.png\")\n",
    "#Image credit (CC-BY-NA): [Karl Rosaen](http://karlrosaen.com/ml/learning-log/2016-06-20/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines with Grid Search\n",
    "\n",
    "Grid Search is a way of testing out our hyperparameters to suss out the most ideal model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First create the pipe or the list of procedures with some defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    PolynomialFeatures(2),\n",
    "    MinMaxScaler(),\n",
    "    SelectPercentile(percentile=20),\n",
    "    RandomForestClassifier(max_depth=7))\n",
    "#pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now add the GridSearch part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time begin    \n",
    "   params = Dict(\"polynomialfeatures__degree\"=> [1, 2, 3],\n",
    "              \"selectpercentile__percentile\"=> [10, 15, 20, 50],\n",
    "              \"randomforestclassifier__max_depth\"=> [5, 7, 9],\n",
    "              \"randomforestclassifier__criterion\"=> [\"entropy\", \"gini\"])\n",
    "\n",
    "    grid = GridSearchCV(pipe, params, cv=5)\n",
    "    fit!(grid, X, y)\n",
    "\n",
    "    print(\"best cross-validation accuracy:\", grid.best_score_)\n",
    "    #print(\"best dataset score: \", grid.grid_scores_)   # Overfitting against entire dataset\n",
    "    print(\"best parameters: $(grid.best_params_)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose best model as overall learning model... THE MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best parameters: $(grid.best_params_)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ideal model to original dataset X not X_selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth=9, criterion=\"entropy\", random_state=1)\n",
    "cv_scores = cross_val_score(model, X, y, scoring=scorer, cv=5)# Stratified kfold done here. \n",
    "println(\" CV scores: \", cv_scores)\n",
    "println(\"Mean score: \", mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that Classifier [1-Malignant ,  0-Benign] works\n",
    "\n",
    "X[50] was Malignant\n",
    "\n",
    "X[11] was Benign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose([X[11,:] X[50,:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r_model = fit!(model, X, y)\n",
    "fit!(model, X, y)\n",
    "\n",
    "#results = predict(model,transpose([X[50,:] X[11,:]]))\n",
    "results = predict(model,transpose([X[50,:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can Serialize the Model and store it for usage in other .jl files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JLD\n",
    "Julia's Data Format\n",
    "\n",
    "uses Hierarchical Data Format 5 (HDF5)\n",
    "https://en.wikipedia.org/wiki/Hierarchical_Data_Format\n",
    "open source file formats (HDF4, HDF5) designed to store and organize large amounts of data. \n",
    "open source file format for storing huge amounts of numerical data. It’s typically used in research applications (meteorology, astronomy, genomics etc.) to distribute and access very large datasets without using a database. One can use HDF5 data format for pretty fast serialization to large datasets. \n",
    "\n",
    "Serialization\n",
    "is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment)\n",
    "This process of serializing an object is also called marshalling an object. The opposite operation, extracting a data structure from a series of bytes, is deserialization (also called unmarshalling).\n",
    "\n",
    "$array = array(\"a\" => 1, \"b\" => 2, \"c\" => array(\"a\" => 1, \"b\" => 2));\n",
    "\n",
    "serialized in JSON to\n",
    "\n",
    "```$json = json_encode($array);```\n",
    "will give you this:\n",
    "\n",
    "{\"a\":1,\"b\":2,\"c\":{\"a\":1,\"b\":2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import JLD, PyCallJLD\n",
    "JLD.save(\"cancer_model.jld\", \"model\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
